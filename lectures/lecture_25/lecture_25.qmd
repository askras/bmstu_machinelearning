---
title: 'Лекция 25: Рекуррентные нейронные сети (RNN)'
subtitle: Машинное обучение и анализ данных
author: Красников А. С.
date: '2025-04-03'
date-modified: last-modified
institute: МГТУ им. Н.Э. Баумана

format:
  revealjs:
    lang: ru
    scrollable: true
    transition: fade
    width: 1920
    height: 1080
---


# Обработка последовательностей

## Входные данные в виде последовательностей

- Динамика цен на акции
- Динамика действий посетителей веб-сайта
- Текст как последовательность слов
- Речь как последовательность звуков
- Видео как последовательность кадров

## Независимая обработка

- Можно обрабатывать каждый элемент последовательности независимо
- Проблема: не учитывает зависимости между элементами
- Пример: разметка частей речи в тексте
  - Слова могут иметь разное значение в зависимости от контекста

![](./img/01-iid-processing.png)


## Использование свёрток

- Можно применить свёртку для учёта контекста
- Ограничения:
  - Учитывается только ограниченный контекст
  - Вычислительная сложность растёт с длиной контекста

![](./img/03-POS-conv-processing.png)

## Рекуррентные сети (RNN)

- Используют скрытое состояние для хранения исторического контекста
$$\mathbf{h}_{t} = g(\mathbf{x}_{t}, \mathbf{h}_{t-1})$$
- Прогноз зависит от текущего наблюдения и скрытого состояния
$$\hat{\mathbf{y}}_{t} = f(\mathbf{x}_{t}, \mathbf{h}_{t})$$
- Скрытое состояние обновляется рекуррентно

![](img/04-RNN-processing.png)


## Проблемы настройки

1. Проблема затухающих градиентов (vanishing gradient problem)
2. Проблема взрывающихся градиентов (exploding gradient problem)

**Решения:**

- Специальные архитектуры (GRU, LSTM)
- Обрезка градиентов (gradient clipping)


## Простейшая рекуррентная сеть (сеть Элмана)


$$
\begin{aligned}
H_{t} & = g(W_{xh} X_{t} + W_{hh}H_{t-1} + \mathbf{b}_{h}\mathbf{e}^\top), \\
Y_{t} & = f(W_{hy}H_{t} + \mathbf{b}_{y} \mathbf{e}^\top),
\end{aligned}
$$

где

- $X_t \in \mathbb{R}^{D \times B}$ --- матрица текущих входов для каждой последовательности (по столбцам);
- $H_t \in \mathbb{R}^{D \times B}$ --- матрица текущих состояний для каждой последовательности (по столбцам);
- $\mathbf{e} = [1, 1, ..., 1 ]^{\top} \in \mathbb{R}^B$ --- домножение вектора на него повторяется $B$ раз.



## Настройка рекуррентной сети

- Обратное распространение ошибки во времени (BPTT)
- Обрезанное BPTT для длинных последовательностей
- Обработка мини-батчей с дополнением токенами [PAD]

![]("img/padding-example.png")


## Регуляризация

- Ограничение числа слоёв и нейронов
- L1/L2 регуляризация
- DropOut:
  - Naive RNN DropOut
  - Variational RNN DropOut

![]("img/05-RNN-DropOut.png")


## Входы и выходы RNN

- Вещественные входы/выходы: используются напрямую
- Категориальные входы: эмбеддинги слов/символов
- Категориальные выходы: SoftMax преобразование

## Рекуррентная сеть как кодировщик

- Последнее скрытое состояние - эмбеддинг последовательности
- Можно получать эмбеддинги:
  - Слов (из букв)
  - Параграфов (из слов)
  - Звуков (из частот)
  - Видео (из фреймов)

# Генерация последовательностей

##  Примеры последовательностей для генерации

Рекуррентную сеть можно использовать для генерации последовательности $\hat{\mathbf{y}}_1\hat{\mathbf{y}}_2,...\hat{\mathbf{y}}_T$, которая может представлять собой:

1. Последовательность чисел (например, изменение температур)
2. Последовательность вещественных векторов (например, цены на акции)
3. Последовательность дискретных объектов (например, последовательность слов)


## Условная генерация

Простая генерация (текстов, временных рядов) не имеет прикладного значения. Однако большую применимость имеет **условная генерация**:

- Генерируется текст по заданной теме (ответ на вопрос)
- Генерируется временной ряд по условию (вероятная динамика будущих цен)

Безусловная генерация разрабатывается как предварительный этап перед условной генерацией.


## Процесс генерации

Для генерации:

1. Подаётся фиксированное начальное состояние $\mathbf{h}_0$
2. Фиксированный вектор начального входа $\hat{\mathbf{y}}_0$

Их можно задавать как:

- Нулевые вектора
- Обучаемые параметры


## Генерация векторов чисел


:::: {.columns}

::: {.column width="50%"}
Алгоритм:

1. Задать $\mathbf{h}_0,\hat{\mathbf{y}}_0$ и $p_{max}$
2. Для каждого $t$:
   - $\mathbf{x}_t := \hat{\mathbf{y}}_{t-1}$
   - $\mathbf{h}_{t} := g(\mathbf{x}_{t}, \mathbf{h}_{t-1})$
   - $\hat{\mathbf{y}}_{t} := f(\mathbf{x}_{t}, \mathbf{h}_{t})$
   - $p_t := s(\mathbf{h}_t)$
3. Остановиться при $p_t < p_{max}$

:::

::: {.column width="50%"}
![](./img/06-RNN-generator.png)
:::

::::


## Генерация дискретных объектов

:::: {.columns}

::: {.column width="50%"}
Для генерации текста:

1. На каждом шаге $t$ сеть выдаёт $V$ рейтингов слов
2. Рейтинги преобразуются в вероятности через SoftMax
3. Генерируется слово:
   - Во время обучения - самое вероятное
   - Во время применения - случайное сэмплирование
:::

::: {.column width="50%"}
![](./img/06-RNN-word-generator.png)
:::

::::


## Остановка генерации

Варианты остановки:

1. По вероятности остановки $p_t = s(\mathbf{h}_t)$
2. По специальному токену [EOS] (end of sequence)


## Обучение генеративной сети

Сеть обучается на фрагментах реальных последовательностей $\mathbf{y}_1 \mathbf{y}_2 ... \mathbf{y}_T$, где целевые значения --- сдвинутая на 1 последовательность.

Функции потерь:

- Регрессия: регрессионные функции
- Классификация: кросс-энтропийные


## Стратегии обучения

**Free run**:

- На вход подаётся сгенерированный элемент
- Соответствует реальному применению
- Сложнее обучать

**Teacher forcing**:

- На вход подаётся реальный элемент
- Проще обучать
- Проблемы при генерации ошибок


## Смесь стратегий

Оптимальные подходы:

1. Сначала teacher forcing, затем free run
2. Одновременное обучение с постепенным уменьшением вероятности teacher forcing


# Оценка качества языковых моделей

## Языковая модель

- Статистически моделирует свойства естественного языка
- Предсказывает вероятности последовательностей слов
- Генерирует осмысленные текстовые данные



## Подход к оценке качества

Для оценки качества:

1. Применяют модель к реальным текстам
2. Анализируют, насколько реальные слова текста вероятны согласно предсказанному распределению

Обозначения:

- Реальный текст: $w_1w_2...w_T$


## Средний логарифм правдоподобия

$$
\begin{aligned}
S(w_1w_2...w_T) &= \frac{1}{T}\log P(w_1w_2...w_T) \\
&=\frac{1}{T}\sum_{t=1}^T \log P(w_t|w_1w_2...w_{t-1})
\end{aligned}
$$

**Чем выше значение**, тем лучше модель согласуется с реальным текстом


## Перплексия (Perplexity)

Исторически более распространенная метрика:

$$
\text{PPL}=e^{-\frac{1}{T}\sum_{t=1}^{T}\ln p\left(w_{t}|w_{t-1}w_{t-2}\ldots w_{1}\right)}
$$

Эквивалентная форма:

$$
PPL = \frac{1}{ \left( \prod_{t=1}^T p(w_t|w_{t-1}w_{t-2}...w_1) \right)^{1/T} }
$$


## Интерпретация перплексии

- **Можно интерпретировать** как среднее число вариантов следующего слова, которые рассматривает модель
- **Чем меньше** перплексия, тем лучше

Диапазон значений:

- Идеал: 1 (вероятность следующего слова = 1)
- Худший случай: $\infty$ (вероятность = 0)
- Базовый уровень: $p = 1/V$ (равномерное распределение по словарю)


## Метрика ROUGE

**ROUGE** (Recall-Oriented Understudy for Gisting Evaluation) - оценивает совпадение n-грамм:

**Варианты:**

- **ROUGE-N**: сравнивает пересечение n-грамм
  - ROUGE-1: сравнение слов
  - ROUGE-2: сравнение биграмм
- **ROUGE-L**: по длине наибольшей общей подпоследовательности


## Дополнительные метрики ROUGE

Другие варианты:

- **ROUGE-W**: взвешенная версия
- **ROUGE-S**: учитывает пропуски

**Важное отличие:**

- Общая подпоследовательность ≠ подстрока
- В подпоследовательности слова могут идти со вставками, но в том же порядке


# Улучшение генерации текста с помощью языковых моделей

- Применимо к:
  - Любым языковым моделям (трансформерам и др.)
  - Генерации любых дискретных объектов (ДНК, пользовательские сессии и т.д.)
- Сеть генерирует текст последовательно, выдавая рейтинги слов на каждом шаге


## Жадный поиск (Greedy Search)

- Простейший подход генерации
- На каждом шаге выбирается слово с наивысшим рейтингом
- Визуализация в виде дерева выбора

## Пример работы жадного поиска

- Генерация 4-буквенного слова из "A" и "M"
- Пошаговый выбор максимального рейтинга:

---

![](./img/01-greedy-search.png)

---

![](./img/02-greedy-search.png)

---

![](./img/03-greedy-search.png)

---

![](./img/04-greedy-search.png)

---

![](./img/05-greedy-search.png)

---

![](./img/06-greedy-search.png)

---

![](./img/07-greedy-search.png)

---

![](./img/08-greedy-search.png)


## Проблема жадного поиска

- Локально оптимальный выбор на каждом шаге
- Глобально последовательность может быть неоптимальной
- Пример: получено "AAAA" с рейтингом 48, но есть лучшие варианты (МАМА)


## Лучевой поиск (Beam Search)

- Выбирается K лучших гипотез на каждом шаге
- K - гиперпараметр метода
- При K=1 сводится к жадному поиску
- При $K\geqslant V^T$ - полный перебор

## Пример работы лучевого поиска (K=2)

- Параллельное развитие 2 лучших гипотез:

---

![](./img/01-beam-search.png)

---

![](./img/02-beam-search.png)

---

![](./img/03-beam-search.png)

---

![](./img/04-beam-search.png)

---

![](./img/05-beam-search.png)

---

![](./img/06-beam-search.png)

---

![](./img/07-beam-search.png)

---

![](./img/08-beam-search.png)

---

- Результат: "AMAM" с рейтингом 60 (лучше жадного 48)

## Ограничения лучевого поиска

- Не обеспечивает полный перебор
- Может пропустить глобально оптимальное решение ("MAMA" с рейтингом 80)
- Чем больше K, тем лучше поиск, но выше вычислительные затраты

## Случайная генерация

- Для разнообразия текстов слова выбираются из распределения
- Управление температурой в SoftMax
- Два подхода:
  - Top-K sampling (фиксированное K)
  - Nucleus sampling (динамическое K по порогу P)

## Рейтинг последовательности

- Оценка как логарифм вероятности последовательности
- Нормировка на длину для поощрения более длинных вариантов
- Дополнительные критерии:
  - Разнообразие (уникальные n-граммы)
  - Естественность (реальные последовательности слов)


# Режимы применения рекуррентных сетей

## Условные обозначения

![](./img/definitions.png)

## Synchronized Many-to-Many

:::: {.columns}

::: {.column width="50%"}

- Классическое применение RNN
- Для каждого входа $x_t$ строится выход $\hat{y}_t$
- Потери вычисляются как среднее по всем элементам последовательности:

$$\mathcal{L}(\widehat{Y},Y)=\frac{1}{T}\sum_{t=1}^{T}\mathcal{L}(\hat{\mathbf{y}}_{t},\mathbf{y}_{t})$$
:::

::: {.column width="50%"}
![](./img/synchronized-many-to-many.png)
:::

::::


## Примеры Synchronized Many-to-Many

- Синхронная разметка частей речи
- Синхронная обработка фреймов видеопотока
- Особенность: длина выходной последовательности равна длине входной


## Many-to-One

На вход поступает последовательность, для которой нужно произвести однократный прогноз.

![](./img/many-to-one.png)


## Примеры Many-to-One

- Анализ тональности текста
- Классификация текста по темам
- Распознавание заболеваний по временным данным
- Классификация видео по категориям

## Реализации Many-to-One

### Учёт последнего прогноза

- Считывается только последний выход
- Подходит для коротких последовательностей

### Учёт всех прогнозов

- Агрегируются выходы всех шагов
- Подходит для длинных последовательностей


## One-to-Many

По эмбеддингу запроса генерируется выходная последовательность.

![](./img/one-to-many.png)

## Примеры One-to-Many
- Генерация музыки по жанру
- Описание изображения текстом
- Генерация видео по начальному фрейму

## Реализации One-to-Many

### Запрос через начальное состояние
- Эмбеддинг задаёт начальное состояние
- Подходит для коротких выходных последовательностей

### Запрос на каждом входе
- Эмбеддинг подаётся на каждом шаге
- Не позволяет забыть запрос



## Many-to-Many (Seq2Seq)

По входной последовательности генерируется выходная последовательность.

![](./img/many-to-many.png)

## Примеры Many-to-Many

- Машинный перевод
- Системы ответов на вопросы
- Распознавание речи
- Генерация речи

## Архитектура Many-to-Many

Состоит из:

1. Кодировщика (encoder) - решает задачу many-to-one
2. Декодировщика (decoder) - решает задачу one-to-many


## Рекомендации по Many-to-Many

- Можно объединить оба подхода:
  - Инициализировать состояние декодировщика последним состоянием кодировщика
  - Конкатенировать последний выход кодировщика к каждому входу декодировщика
- Для остановки генерации использовать:
  - Специальный токен [EOS]
  - Порог вероятности конца генерации


# Усложнения рекуррентных сетей

## Многослойные сети

:::: {.columns}

::: {.column width="50%"}
- Аналогично свёрточным сетям, можно наслаивать несколько рекуррентных сетей
- Позволяет извлекать более сложные зависимости
- Называется multilayer RNN или stacked RNN

:::

::: {.column width="50%"}
![](./img/RNN-Mutlilayer.png)
:::
::::

## Комбинация низкоуровневых и высокоуровневых признаков

:::: {.columns}

::: {.column width="50%"}
- Добавление тождественных преобразований с низких слоёв на высокие
- Входы вышестоящей сети содержат выходы нескольких нижестоящих блоков
- Облегчает настройку сети методом обратного распространения ошибки

:::

::: {.column width="50%"}
![](./img/RNN-mutlilayer-skip-connections-vertical.png)
:::
::::


## Clockwork RNN

:::: {.columns}

::: {.column width="50%"}
- Обработка последовательностей на разных временных масштабах
- Рекуррентные блоки обновляются с различной частотой
- Лучшая суммаризация информации о длинных последовательностях

:::

::: {.column width="50%"}
![](./img/RNN-mutlilayer-skip-connections-horizontal.png)
:::
::::


## Двунаправленная сеть

:::: {.columns}

::: {.column width="50%"}
- Состоит из 2 независимых рекуррентных сетей:
  - Обработка слева-направо
  - Обработка справа-налево
- Состояния конкатенируются для кодирования левого и правого контекста
:::

::: {.column width="50%"}
![](./img/RNN-bidirectional.png)
:::
::::


## Динамическая сеть


:::: {.columns}

::: {.column width="50%"}
- Веса для пересчёта состояний делаются динамическими
- Предсказываются дополнительной сетью (гиперсетью)
- Реализует мягкую общность весов (soft weight sharing)
:::

::: {.column width="50%"}
![](./img/RNN-dynamic.png)
:::
::::


# Модели LSTM и GRU


## Введение

- LSTM и GRU - усложнённые версии классической рекуррентной сети
- Специально разработаны для обработки длинных последовательностей
- Лучше запоминают историю из ранее виденных наблюдений
- Преобразуют входную последовательность в последовательность состояний

---

## Гейты в нейронных сетях

- Улучшенная память достигается заменой перемножений матриц на взвешенные суммы
- Гейты (вентили) - вектора чисел в отрезке [0,1]
- Управляют потоками данных:
  - 0 - гейт закрыт (информация блокируется)
  - 1 - гейт открыт (информация проходит)

---

## Пример работы гейтов

```python
# Пример данных
x_t = [1, 2, 3, 4]
h_t_1 = [10, 20, 30, 40]

# Гейт
g_t = [1, 0, 1, 0]

# Обновление состояния
h_t = g_t * x_t + (1 - g_t) * h_t_1
# Результат: [1, 20, 3, 40]
```


## Реализация гейтов

- Гейты реализуются линейными слоями с сигмоидной функцией активации
- Открытые элементы гейта приводят к обновлению информации
- Закрытые элементы сохраняют информацию
- Позволяют учитывать долгосрочные зависимости в данных


## Модель GRU

Предложена в 2014 году (Cho et al.)

Формулы:

\small
$$
\begin{aligned}
\mathbf{r_t} &= \sigma(W_r\mathbf{x_t} + U_r\mathbf{h_{t-1}} + \mathbf{b_r}) & \text{reset gate} \\
\mathbf{z_t} &= \sigma(W_z\mathbf{x_t} + U_z\mathbf{h_{t-1}} + \mathbf{b_z}) & \text{update gate} \\
\tilde{\mathbf{h}}_t &= \text{tanh}(W_h\mathbf{x_t} + U_h(\mathbf{r_t \odot h_{t-1}}) + \mathbf{b_h}) & \text{proposal} \\
\mathbf{h}_t &= (1-\mathbf{z_t})\odot\mathbf{h_{t-1}} + \mathbf{z_t} \odot \tilde{\mathbf{h}}_t & \text{state}
\end{aligned}
$$


## Параметры GRU

- Матрицы: $W_z, U_z, W_r, U_r, W_h, U_h$
- Векторы: $\mathbf{b}_z, \mathbf{b}_r, \mathbf{b}_h$
- Два гейта:
  - Гейт перезапуска (reset gate)
  - Гейт обновления (update gate)


## Модель LSTM

Предложена в 1997 году (Hochreiter)

Формулы:

\tiny
$$
\begin{aligned}
\mathbf{f_t} &= \sigma(W_f\mathbf{x_t} + U_f\mathbf{h_{t-1}} + \mathbf{b_f}) & \text{forget gate} \\
\mathbf{i_t} &= \sigma(W_i\mathbf{x_t} + U_i\mathbf{h_{t-1}} + \mathbf{b_i}) & \text{input gate} \\
\mathbf{o_t} &= \sigma(W_o\mathbf{x_t} + U_o\mathbf{h_{t-1}} + \mathbf{b_o}) & \text{output gate} \\
\tilde{\mathbf{c}}_t &= \text{tanh}(W_c\mathbf{x_t} + U_c\mathbf{h_{t-1}} + \mathbf{b_c}) & \text{proposal} \\
\mathbf{c_t} &= \mathbf{f_t} \odot \mathbf{c_{t-1}} + \mathbf{i_t} \odot \tilde{\mathbf{c}}_t & \text{memory cell} \\
\mathbf{h_t} &= \mathbf{o_t} \odot \text{tanh}(\mathbf{c_t}) & \text{state}
\end{aligned}
$$


## Параметры LSTM

- Матрицы: $W_f, U_f, W_i, U_i, W_o, U_o, W_c, U_c$
- Векторы: $\mathbf{b}_f, \mathbf{b}_i, \mathbf{b}_o, \mathbf{b}_c$
- Три гейта:
  - Гейт забывания (forget gate)
  - Входной гейт (input gate)
  - Выходной гейт (output gate)
- Дополнительно: вектор памяти (memory cell)


## Особенности LSTM

- Forget gate: на самом деле сохраняет память при значении 1
- Рекомендуется инициализировать $\mathbf{b}_f$ единицами
- LSTM сложнее GRU, но была предложена на 17 лет раньше
- Обе модели могут моделировать схожие поведения


## Преимущества LSTM/GRU

1. Запоминать информацию, которая была очень давно
2. Игнорировать нерелевантную информацию
3. Перезапускаться по достижении логического разрыва



## Примеры генерации текстов

![Генерация текста в стиле Шекспира](./img/RNN-Shakespeare.png)


## Примеры генерации кода

![Генерация исходного кода Linux](./img/RNN-linux-code.png)


## Альтернативные подходы

1. Инициализация матрицы пересчета единичной матрицей (Le et al., 2015)
2. Разделение состояния на быструю и медленную компоненты (Mikolov et al., 2014)


## Проблема длинных последовательностей

- Даже LSTM/GRU не могут полноценно обработать очень длинные последовательности
- Необходимо хранить всю историю в векторе фиксированного размера
- **Решение**: механизм внимания (attention mechanism)
  - Позволяет обращаться к любому элементу входной последовательности напрямую
  - Цена - повышенные расходы на память и вычисления


# Механизм внимания

## Идея внимания

:::: {.columns}

::: {.column width="50%"}

- Позволяет декодировщику смотреть на все токены входной последовательности
- Использует три сущности:
  - Запросы (queries)
  - Ключи (keys)
  - Значения (values)
:::
::: {.column width="50%"}
![Архитектура с вниманием](./img/many-to-many-collected.png)
:::
::::

## Принцип работы внимания

:::: {.columns}

::: {.column width="50%"}

- Вдохновлено SQL: `select VALUE where KEY=QUERY`
- В нейросетях используется мягкое внимание (soft attention)
- Вычисляется функция сходства запроса с каждым ключом
- Значения усредняются пропорционально сходству
:::
::: {.column width="50%"}
![Принцип работы QKV](./img/QKV-soft-select.png)
:::
::::


## Детали реализации
:::: {.columns}

::: {.column width="50%"}
1. Кодировщик - двунаправленная RNN
2. Для каждого состояния декодировщика:
   - Вычисляется соответствие с ключами
   - Вычисляются веса через SoftMax
   - Создается контекстный вектор
   - Обновляется состояние декодировщика
:::
::: {.column width="50%"}
![Схема работы seq2seq с вниманием](./img/seq2seq-attention.png)
:::
::::

## Методы вычисления соответствий

| Метод                | Формула                                                                 |
|-----------------------|-------------------------------------------------------------------------|
| Dot-product          | $\mathbf{s}^{T}\mathbf{h}$                                              |
| Scaled dot-product   | $\mathbf{s}^{T}\mathbf{h}/\sqrt{d}$                                     |
| Content-based        | $\mathbf{s}^{T}\mathbf{h}/(\|\mathbf{s}\| \|\mathbf{h}\|)$             |
| Additive            | $w^{T}\tanh(W_{1}\mathbf{s}+W_{2}\mathbf{h})$                          |
| Multiplicative      | $\mathbf{s}^{T}W\mathbf{h}$                                             |
| Feed-forward        | $\text{MLP}(\mathbf{s},\mathbf{h})$                                    |


## Результаты
:::: {.columns}

::: {.column width="50%"}
- Значительное улучшение качества перевода (BLEU)
- Особенно заметно для длинных последовательностей
- RNNsearch (с вниманием) превосходит RNNenc (без внимания)
:::
::: {.column width="50%"}
![Сравнение качества](./img/seq2seq-attention-quality.png)
:::

::::

## Интерпретируемость
:::: {.columns}

::: {.column width="50%"}
- Можно визуализировать веса внимания
- Матрица внимания обычно близка к диагональной
- Позволяет понять, как сеть "фокусируется" на входных словах
:::

::: {.column width="50%"}
![Матрица внимания](./img/seq2seq-attention-alphas.png)
:::

::::



## Другие применения

### Распознавание речи

- Вход: спектрограмма звука
- Выход: распознанный текст
- Матрица внимания должна быть диагональной

### Описание изображений

- Вход: изображение (представление от CNN)
- Выход: текстовое описание
- Можно визуализировать, на какие части изображения смотрела сеть

---

- Механизм внимания значительно улучшает обработку длинных последовательностей
- Повышает качество моделей (перевод, распознавание речи и др.)
- Улучшает интерпретируемость моделей
- Основные недостатки: повышенные требования к вычислениям и памяти



## Литература

Вот оформленный список литературы по ГОСТ 7.1-2003 (для библиографических ссылок) и ГОСТ Р 7.0.5-2008 (для электронных ресурсов):

1. Elman J. L. Finding structure in time // Cognitive Science. 1990. Vol. 14, no. 2. P. 179-211.

2. Werbos P. J. Backpropagation through time: what it does and how to do it // Proceedings of the IEEE. 1990. Vol. 78, no. 10. P. 1550-1560.

3. Zaremba W., Sutskever I., Vinyals O. Recurrent neural network regularization // arXiv preprint arXiv:1409.2329. 2014.

4. Gal Y., Ghahramani Z. A theoretically grounded application of dropout in recurrent neural networks // Advances in neural information processing systems. 2016. P. 1019-1027.

5. Эффективность рекуррентных нейронных сетей [Электронный ресурс] // Karpathy A. URL: http://karpathy.github.io/2015/05/21/rnn-effectiveness/ (дата обращения: 01.04.2025).

6. The Unreasonable Effectiveness of Recurrent Neural Networks [Электронный ресурс] / A. Karpathy. URL: http://karpathy.github.io/2015/05/21/rnn-effectiveness/ (дата обращения: 01.04.2025).

7. Hihi S., Bengio Y. Hierarchical recurrent neural networks for long-term dependencies // Advances in neural information processing systems. 1995. P. 493-499.

8. Koutnik J. et al. A clockwork RNN // arXiv preprint arXiv:1402.3511. 2014.

9. Schuster M., Paliwal K. K. Bidirectional recurrent neural networks // IEEE Transactions on Signal Processing. 1997. Vol. 45, no. 11. P. 2673-2681.

10. Ha D., Dai A., Le Q. V. Hypernetworks // arXiv preprint arXiv:1609.09106. 2016.

11. Cho K. et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation // arXiv preprint arXiv:1406.1078. 2014.

12. Hochreiter S., Schmidhuber J. Long short-term memory // Neural computation. 1997. Vol. 9, no. 8. P. 1735-1780.

13. Karpathy A. The Unreasonable Effectiveness of Recurrent Neural Networks [Электронный ресурс]. URL: http://karpathy.github.io/2015/05/21/rnn-effectiveness/ (дата обращения: 01.04.2025).

14. Le Q. V. et al. A simple way to initialize recurrent networks of rectified linear units // arXiv preprint arXiv:1504.00941. 2015.

15. Mikolov T. et al. Learning longer memory in recurrent neural networks // arXiv preprint arXiv:1412.7753. 2014.

16. Bahdanau D., Cho K., Bengio Y. Neural machine translation by jointly learning to align and translate // arXiv preprint arXiv:1409.0473. 2014.

17. Augmented Neural Networks [Электронный ресурс] / Distill. 2016. URL: https://distill.pub/2016/augmented-rnns/ (дата обращения: 01.04.2025).
