---
title: 'Лекция 24: Эмбеддинги слов в машинном и глубоком обучении'
subtitle: Машинное обучение и анализ данных
author: Красников А. С.
date: '2025-03-27'
date-modified: last-modified
institute: МГТУ им. Н.Э. Баумана
format:
  revealjs:
    scrollable: true
    center-title-slide: false
    transition: fade
    width: 1920
    height: 1080
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.7
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---


# Задачи автоматической обработки текста  (natural language processing, NLP) {.center}

## Семантический анализ и понимание текста

- **Распознавание именованных сущностей (NER)**:   
  - Выявление имен людей, организаций, дат, локаций 
(например, _«Иван Петров работает в Google» → [Person: Иван Петров, Org: Google]_).
- **Извлечение ключевых слов/фраз**:
  - Определение главных тем текста.
- **Анализ тональности (Sentiment Analysis)**: 
  - Определение эмоциональной окраски (позитив/негатив/нейтрал)
- **Разрешение кореференции**: 
  - Понимание, какие слова относятся к одному объекту (_«Маша пришла. Она улыбалась» → «Она» = Маша_).
- **Семантическая роль**: 
  - Определение отношений между словами (кто что сделал, кому, когда).

## Классификация и категоризация

- **Классификация текста**: 
  - Отнесение текста к категориям (спам/не спам, темы: политика, спорт).
- **Тематическое моделирование**: 
  - Автоматическое выявление скрытых тем в документах (например, LDA).
- **Определение языка текста**: 
  - Распознавание языка, на котором написан текст.

## **Генерация и преобразование текста**

- **Машинный перевод**: 
  - Перевод текста между языками (например, Google Translate).
- **Генерация текста**: 
  - Создание осмысленного текста нейросетями (GPT, ChatGPT).
- **Автоматическое реферирование**: 
  - Сокращение текста до ключевых идей.
- **Перефразирование**: 
  - Переписывание текста с сохранением смысла.

## **Извлечение информации**

- **Поиск информации (Information Retrieval)**: 
  - Поиск релевантных документов по запросу (например, поисковики).
- **Вопросно-ответные системы (QA)**: 
  - Автоматический ответ на вопросы на основе текста (_«Столица Франции?» → «Париж»_).
- **Сбор структурированных данных**: 
  - Извлечение фактов из текста (например, дат событий, связей между объектами).

## **Работа с диалогами и речью**

- **Обработка диалогов (Dialog Systems)**: 
  - Чат-боты, голосовые ассистенты (Siri, Alexa).
- **Распознавание речи (ASR)**: 
  - Преобразование аудио в текст.
- **Синтез речи (TTS)**: 
  - Преобразование текста в аудио.

## **Дополнительные задачи**

- **Определение авторства**: 
- Анализ стиля текста для идентификации автора.
- **Обнаружение плагиата**: 
  - Поиск заимствований в тексте.
- **Анализ графов знаний**: 
  - Построение связей между сущностями (например, Wikidata).


# Представление текста {.center}


## Восприятие текста

Текст --- последовательность слов.

- У человека сразу возникают ассоциации:
  - &laquo;Мама мыла раму&raquo;
  - &laquo;Посадил дед репку&raquo;
  - &laquo;Смеркалось...&raquo;
- Для &laquo;компьютера&raquo; нужно представить в числовом виде.


## Эмбеддинги

:::: {.columns}

::: {.column width="40%"}
**Цель**: Представление слов в виде векторов фиксированной размерности.  
  $$ w \to \mathbf{v}_w \in \mathbb{R}^D $$  
  где $D$ — размерность эмбеддинга.
:::

::: {.column width="60%"}
![Пример эмбеддингов](./img/word-embeddings.png){.scratch}
:::

::::


## Особенности представления текста
### Служебные токены

- `PAD`: Отсутствие слова
- `BOS`: Начало последовательности
- `EOS`: Конец последовательности

### Токенизация
Текст может обрабатываться как:

- Последовательность **слов**
- Последовательность **символов**
- Последовательность **семантических групп**  

## Пример

:::: {.columns}

::: {.column width="50%"}
```text
Жила-была в одной деревне девочка красоты невиданной:
мать любила её без памяти, а бабушка и того больше.

Сшила как-то раз бабушка любимой внучке шапочку
красного цвета и так сильно она девочке понравилась,
что и снимать не хотелось. 
Всюду ходила она в своей шапочке, потому и стали называть 
её Красной Шапочкой.
```
:::

::: {.column width="50%"}
```text
[Жила-была] [в] [одной деревне] [девочка] [красоты невиданной]:
[мать] [любила] [её] [без памяти][,] [а] [бабушка] [и того больше][.]

[Сшила] [как-то раз] [бабушка] [любимой] [внучке] [шапочку]
[красного] [цвета] [и] [так] [сильно] [она] [девочке] [понравилась],
[что] [и] [снимать] [не] [хотелось]. 
[Всюду] [ходила] [она] [в] [своей] [шапочке][,] [потому] [и] [стали] [называть] 
[её] [Красной Шапочкой][.]
```
:::

::::

- `[Жила-была]` --- устойчивое выражение
- `[красоты невиданной]` --- устойчивое выражение
- `[как-то раз] ` --- устойчивое выражение
- `[Красной Шапочкой]` --- единая сущность


# Кодирование {.center}

## Обозначения

- $v_i$​ --- эмбеддинг, отвечающий слову $i$, например 100001-му в словаре;

- $v^j$ --- значение вектора эмбеддинга $v$ на $j$-й позиции, $j=1,2,..., D$.


## Ручное кодирование

Каждое слово можно представить вектором признаков, описывающих это слово, например
 
- $v^1$: часть речи
- $v^2$: род (мужской/женский/средний - для существительных и прилагательных)
- $v^3$: время (прошедшее/настоящее/будущее - для глаголов)
- $v^4$: индикатор, что слово начинается с заглавной буквы
- $v^5$: количество букв
- $v^6$: категория: машинное обучение, физика, биология, ...
- $v^7$: подкатегория: обучение с учителем, без учителя, частичное обучение, ...
- ...

## Недостатки ручного кодирования

- Требует лингвистических словарей
- Не работает с новыми словами
- Не учитывает многозначность (например, "замок" как крепость и устройство)

## One-hot кодирование

Пример для фразы "место было уютным и комфортным": 

| Слово       | Эмбеддинг          |
|-------------|--------------------|
| место       | [1,0,0,0,0]        |
| было        | [0,1,0,0,0]        |
| уютным      | [0,0,1,0,0]        |
| и           | [0,0,0,1,0]        |
| комфортным  | [0,0,0,0,1]        |


## Недостатки One-hot кодирования

- Высокая размерность
- Нет информации о семантической близости (синонимы далеки друг от друга)


## Автоматическое построение эмбеддингов
### Дистрибутивная гипотеза
> "Слова, близкие по смыслу, часто встречаются совместно."


Человек, читающий текст с новыми для него терминами, восстанавливает их примерный смысл по контексту.

**Пример контекста**:  

> "Современная генетика использует методы исследования **гаплотипов**..."  

Эмбеддинг &laquo;гаплотипа&raquo; будет близок к &laquo;генетике&raquo;, &laquo;эволюции&raquo;.


## Ключевые свойства эмбеддингов

1. **Автоматизация**: Не требуют ручной разметки.
2. **Семантическая близость**:  
   $$ \text{sim}(\mathbf{v}_{\text{уютный}}, \mathbf{v}_{\text{комфортный}}) \gg \text{sim}(\mathbf{v}_{\text{стол}}, \mathbf{v}_{\text{облако}}) $$
3. **Универсальность**: Применимы к:
   - Веб-сессиям пользователей
   - Генетическим последовательностям
   - Социальным графам


## Пример использования
**Задача классификации отзывов**:  
- Фраза 1: "Номера оказались очень **уютными**"  
- Фраза 2: "Номера в отеле оказались очень **комфортными**"  

Благодаря близким эмбеддингам синонимов, модель даст схожие предсказания для обеих фраз.


# Методы, основанные на совстречамости слов {.center}

## Матрица совстречаемости

**Цель**: Представление слов через частоту совстречаемости с другими словами в тексте

**Алгоритм**:

1. Создать словарь уникальных слов
2. Инициализировать матрицу $M \in \mathbb{R}^{S \times S}$ нулями
3. Для каждого слова в тексте:
  - Анализировать контекст (окно $\pm K$ слов)
  - Увеличивать счетчики в матрице для слов контекста

**Формула**:
$$ M[w_t, u] += 1 \quad \text{для каждого } u \in \text{context}(w_t)$$


## Пример расчёта

**&laquo;Кошка любит внимание. Кошка любит нежность. Кошка любит ласку.&raquo;**

**Матрица совстречаемости** ($K=2$)

![](./img/sliding-window-on-text.png)

|          | кошка | любит | внимание | нежность | ласку |
|:---------|:-----:|:-----:|:--------:|:--------:|:-----:|
| кошка    |       | 5     | 2        | 2        | 1     |
| любит    | 5     |       | 2        | 2        | 1     |
| внимание | 2     | 2     |          |          |       |
| нежность | 2     | 2     |          |          |       |
| ласку    | 1     | 1     |          |          |       |

## Учёт расстояния между словами

- **Веса связей**: Близкие слова учитываются с большим весом
- **Пример** для окна $\pm 3$:

![](./img/HALL-method-example.png)


## Разделение левого и правого контекста

**Матрица $M \in \mathbb{R}^{S \times 2S}$**:

|          | кошка L | любит L | внимание L | нежность L | ласку L | кошка R | любит R | внимание R | нежность R | ласку R|
|:---------|:-------:|:-------:|:----------:|:----------:|:-------:|---------|:-------:|:----------:|:----------:|:------:|
| кошка    |         | 2       | 1          | 1          |         |         | 3       | 1          | 1          | 1      |
| любит    | 3       |         | 1          | 1          |         | 2       |         | 1          | 1          | 1      |
| внимание | 1       | 1       |            |            |         | 1       | 1       |            |            |        |
| нежность | 1       | 1       |            |            |         | 1       | 1       |            |            |        |
| ласку    | 1       | 1       |            |            |         |         |         |            |            |        |


**Преимущество**:

- Отдельный учет левого (L) и правого (R) контекста
- Улучшенное представление синтаксических связей


## Снижение влияния частых слов

- **Проблема**: Стоп-слова (и, но, он) искажают статистику
- **Решение**: 
  $$ M_{ij} \to \log(M_{ij} + 1) $$
- **Эффект**: Снижение веса часто встречающихся сочетаний


## Игнорирование случайных совместных встреч

Положительная поточечная взаимная информация (Positive Pointwise Mutual Information, PPMI)
$$
\text{PPMI}(w_i, w_j) = \max\left(\log\frac{P(w_i,w_j)}{P(w_i)P(w_j)}, 0\right)
$$

**где**:

- $P(w_i)$ - вероятность слова $w_i$
- $P(w_i,w_j)$ - совместная вероятность слов

**Преимущество**: Игнорирует случайные совстречаемости


## Влияние ширины контекста

| Узкий контекст ($K=2$)         | Широкий контекст ($K=5$)      |
|-------------------------------|-------------------------------|
| Улавливает синтаксис           | Отражает общий смысл          |
| Пример: "собака → лает"        | Пример: "собака → питомец"    |


## Ограничения

1. Высокая размерность ($S\times S$)
2. Редкие события ухудшают качество
3. Требует последующего снижения размерности (например, SVD)


# Латентный семантический анализ (LSA){.center}

## Проблема высокоразмерных эмбеддингов
- Размерность эмбеддингов: $S$ или $2S$ (число уникальных слов)
- **Недостатки**:
  1. Вычислительная сложность
  2. Риск переобучения
  3. Шум из-за редких совстречаемостей

**Решение**: Сжатие до $D \sim 300$ размерностей

---

## Усечённое сингулярное разложение (Truncated SVD)
**Формула**:
$$
M \approx U \cdot \Sigma \cdot V^T
$$

**где**:

- $U, V \in \mathbb{R}^{S \times D}$ - ортогональные матрицы
- $\Sigma = \text{diag}\{\sigma_1, \sigma_2, ..., \sigma_D\}$ - сингулярные числа
- $D$ - целевая размерность (гиперпараметр)

![](./img/SVD.png)



## Геометрическая интерпретация

- **Строки $V^T$**: Базисные векторы &laquo;семантических тем&raquo;"
- **$\sigma_i$**: Важность каждой темы
- **Строки $U$**: Коэффициенты сочетания тем для каждого слова

**Пример тем**:

1. Политика
2. Экономика
3. Спорт
4. Культура


## Алгоритм построения LSA-эмбеддингов

1. Создать матрицу совстречаемости $M$
2. Применить Truncated SVD:
   $$ M = U\Sigma V^T $$
3. Использовать строки $U$ как эмбеддинги слов

**Результат**:

- $D$-мерные векторы
- Семантическое сходство через косинусную близость

---

## Классический LSA для документов

1. **Представление документов**:
   - TF-IDF векторы размерности $S$
2. **Матрица документов**:
   $$ M \in \mathbb{R}^{N \times S} $$
3. **Применение SVD**:
   $$ M = U\Sigma V^T $$
4. **Эмбеддинг документа**:
   - Строки $U$ размерности $D$

**Применение**:

- Кластеризация документов
- Поиск схожих статей


## Сравнение подходтов

| Параметр           | LSA для слов      | Классический LSA |
|--------------------|-------------------|------------------|
| Объект анализа     | Отдельные слова   | Целые документы  |
| Размерность входа  | $S$ или $2S$      | $S$ (TF-IDF)     |
| Матрица разложения | Совстречаемости   | Документ-термин  |
| Результат          | Семантика слов    | Тематика текстов |


## Преимущества LSA

1. Снижение размерности
2. Устранение шума
3. Выявление скрытых семантических связей
4. Эффективность для разреженных данных

---

## Ограничения LSA
1. Линейность модели
2. Потеря локального контекста
3. Сложность интерпретации тем
4. Зависимость от качества исходной матрицы


# Word2vec {.center}


## Основные подходы Word2vec

:::: {.columns}

::: {.column width="40%"}
- Две ключевые архитектуры:
  - **Skip-Gram**: предсказывает контекстные слова по центральному
  - **CBOW**: предсказывает центральное слово по контексту
- Каждому слову соответствуют:
  - Входной эмбеддинг $\mathbf{u}_w \in \mathbb{R}^D$
  - Выходной эмбеддинг $\mathbf{v}_w \in \mathbb{R}^D$
- Размерность $D \sim 300$
:::

::: {.column width="60%"}
![](./img/sliding-window-on-text.png)
:::

::::


## Модель Skip-Gram

**Цель**: максимизация вероятности контекстных слов:
$$\frac{1}{T}\sum_{t=1}^{T}\sum_{-K\le i\le K,\,i\ne0}\ln p(w_{t+i}|w_{t})\to\max_{\theta}$$

**Формула вероятности**:
$$p(w_{t+i}|w_{t})=\frac{\exp\left(\mathbf{u}_{w_{t}}^{T}\mathbf{v}_{w_{t+i}}\right)}{\sum_{w'=1}^{S}\exp\left(\mathbf{u}_{w_{t}}^{T}\mathbf{v}_{w'}\right)}$$

**Результат**: входные эмбеддинги $\{\mathbf{u}_w\}_w$



## Модель CBOW

**Цель**: предсказание центрального слова по контексту:
$$\frac{1}{T}\sum_{t=1}^{T}\ln p(w_{t}|w_{t-K},..w_{t-1},w_{t+1},...w_{t+K})\to\max_{\theta}$$

**Контекстный эмбеддинг**:
$$\mathbf{u}_{c}=\sum_{-K\le i\le K,\,i\ne0}\mathbf{u}_{w_{t+i}}$$

**Формула вероятности**:
$$p(w_{t}|w_{t-c},..w_{t-1},w_{t+1},...w_{t+c})=\frac{\exp\left(\mathbf{u}_{c}^{T}\mathbf{v}_{w_{t}}\right)}{\sum_{w'=1}^{S}\exp\left(\mathbf{u}_{c}^{T}\mathbf{v}_{w'}\right)}$$

**Результат**: выходные эмбеддинги $\{\mathbf{v}_w\}_w$


## Регулярности в эмбеддингах

[http://epsilon-it.utu.fi/wv_demo/](http://epsilon-it.utu.fi/wv_demo/)

$$\mathbf{v}_{\text{king}}-\mathbf{v}_{\text{queen}} \approx \mathbf{v}_{\text{man}}-\mathbf{v}_{\text{woman}}$$
$$\mathbf{v}_{\text{Paris}}-\mathbf{v}_{\text{France}} \approx \mathbf{v}_{\text{Rome}}-\mathbf{v}_{\text{Italy}}$$

$$\mathbf{v}_{\text{car}}-\mathbf{v}_{\text{cars}} \approx \mathbf{v}_{\text{aplle}}-\mathbf{v}_{\text{apples}}$$
$$\mathbf{v}_{\text{doctor}}-\mathbf{v}_{\text{hospital}} \approx \mathbf{v}_{\text{teacher}}-\mathbf{v}_{\text{school}}$$

:::: {.columns}

::: {.column width="40%"}

![](./img/Word2Vec-regularities-1.png)

:::

::: {.column width="60%"}

![](./img/Word2Vec-regularities-2.png)
:::

::::


## FastText

**Особенности**:

- Представление слов через n-граммы символов (3 ≤ n ≤ 6)
- Пример для "person": `[pe, per, ers, rso, son, on]`
- Эмбеддинг слова: сумма эмбеддингов n-грамм

$$\mathbf{v}_w\to \sum_{\mathbf{e}\in\text{n-grams}(w)} \mathbf{e}_{j}$$

**Преимущества**:

- Работа с опечатками и новыми словами
- Учет морфологии (напр., run → running, runner)


# Оптимизация Skip-Gram {.center}

## Проблема вычислений в Skip-Gram

**Основная сложность**:

$$p(w_{t+i}|w_{t})=\frac{\exp\left(\mathbf{u}_{w_{t}}^{T} \mathbf{v}_{w_{t+i}}\right)}{\color{red}{\sum_{w=1}^{S}\exp\left(\mathbf{u}_{w_{t}}^{T} \mathbf{v}_{w}\right)}}$$

- Вычисление знаменателя требует $O(S)$ операций
- Для больших словарей ($S \sim 10^5-10^6$) это непрактично

**Решение**:

- Иерархический SoftMax
- Негативное сэмплирование



## Иерархический SoftMax

:::: {.columns}

::: {.column width="60%"}
**Принцип работы**:

- Построение бинарного дерева Хаффмана с $S$ листьями
- Глубина дерева: $O(\log_2 S)$
- Вероятность вычисляется через путь от корня к листу

**Формулы переходов**:
$$
\begin{aligned}
p(\text{left}|j) &= \sigma\left(\mathbf{v}_{j}^{T} \mathbf{u}_{w_I}\right) \\
p(\text{right}|j) &= \sigma\left(-\mathbf{v}_{j}^{T} \mathbf{u}_{w_I}\right)
\end{aligned}
$$

**Особенности**:

- Использует несбалансированное дерево для частых слов
- Экономия вычислений до $O(\log S)$

:::

::: {.column width="40%"}
![](./img/hierarchical-softmax-binary-tree.png)
:::

::::



## Негативное сэмплирование

**Альтернативный подход**:

- Для каждого позитивного примера $(w_t, w_{t+i})$ выбираем $K$ негативных примеров
- Целевая функция:
$$
\ln\sigma(\mathbf{u}_{w_t}^T\mathbf{v}_{w_{t+i}}) + \sum_{k=1}^K \ln\sigma(-\mathbf{u}_{w_t}^T\mathbf{v}_{w_{j(k)}})
$$

**Сэмплирование слов**:

- Вероятность сэмплирования: $p(w) \propto \text{freq}(w)^{3/4}$
- Типичные значения $K$: 2-20

**Преимущества**:

- Одна итерация за $O(K)$
- Проще в реализации
- Сопоставимое качество с Hierarchical SoftMax



## Сравнение методов

| Параметр               | Hierarchical SoftMax | Negative Sampling |
|------------------------|----------------------|-------------------|
| Сложность итерации     | $O(\log S)$          | $O(K)$            |
| Качество эмбеддингов   | Высокое              | Сопоставимое      |
| Популярность           | Умеренная            | Широкая           |
| Рекомендуемый $K$      | -                    | 2-20              |



# Эмбеддинги параграфов{.center}

## Введение в эмбеддинги параграфов

- Эмбеддинги могут представлять не только слова, но и **текстовые фрагменты**:
  - Предложения
  - Параграфы
  - Документы
- Применение:
  - Классификация текста
  - Поиск переформулировок
  - Поисковые системы
  - Семантический анализ

**Пример**:  
*"Казнить нельзя помиловать"*  
Зависит от позиции запятой

---

## Метод PV-DM (Distributed Memory)

:::: {.columns}

::: {.column width="60%"}
- Расширение модели CBOW (Word2Vec)
- Особенности:
  - Использует **эмбеддинг параграфа** $\mathbf{e}_p \in \mathbb{R}^D$
  - Комбинирует:
    1. Среднее эмбеддингов контекстных слов
    2. Эмбеддинг всего параграфа
- Варианты реализации:
  - Конкатенация векторов
  - Взвешенное усреднение
:::

::: {.column width="40%"}
![](./img/PV-DM.png)
:::

::::


## Метод PV-DBOW (Distributed Bag of Words)

:::: {.columns}

::: {.column width="60%"}
![Архитектура PV-DBOW](./img/PV-DBOW.png)
:::

::: {.column width="40%"}
- Аналог Skip-Gram (Word2Vec)
- Особенности:
  - Предсказывает **все слова окна**
  - Использует эмбеддинг параграфа как контекст
- Преимущества:
  - Эффективен для небольших данных
  - Быстрее вычисляется
:::

::::



## Ограничения методов

**Основные ограничения**:

1. Требуют предварительного знания текстов
2. Не подходят для новых документов без дообучения
3. Ограниченный контекст обработки

**Альтернативы**:

- Использование RNN/LSTM сетей
- Трансформерные модели (BERT, GPT)
- Динамическое дообучение моделей



# Литература{.center}

---
Оформленный список литературы по ГОСТ 7.1-2003 (актуальная редакция на 2023 год, так как ГОСТ 2007 не существует):

---

1. **Lund K., Burgess C.** [Producing high-dimensional semantic spaces] Создание высокоразмерных семантических пространств // *Behavior Research Methods, Instruments & Computers*. 1996. Vol. 28. Pp. 203–208. URL: https://link.springer.com/content/pdf/10.3758/BF03204766.pdf (дата обращения: 15.10.2023).

2. **Латентно-семантический анализ** // Deep Machine Learning: образовательная платформа. URL: https://deepmachinelearning.ru/docs/Neural-networks/Embeddings/Latent-semantic-analysis (дата обращения: 15.10.2023).

3. **Mikolov T.** Efficient estimation of word representations in vector space // *arXiv.org*. 2013. arXiv:1301.3781. URL: https://arxiv.org/abs/1301.3781 (дата обращения: 15.10.2023).

4. **Mikolov T., Sutskever I., Chen K., Corrado G.S., Dean J.** Distributed representations of words and phrases and their compositionality // *Advances in Neural Information Processing Systems (NeurIPS)*. 2013. Pp. 3111–3119. URL: https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html (дата обращения: 15.10.2023).

5. **Bojanowski P., Grave E., Joulin A., Mikolov T.** Enriching word vectors with subword information // *arXiv.org*. 2017. arXiv:1607.04606. URL: https://arxiv.org/pdf/1607.04606 (дата обращения: 15.10.2023).

6. **Le Q.V., Mikolov T.** Distributed representations of sentences and documents // *Proceedings of the 31st International Conference on Machine Learning (ICML)*. 2014. Pp. 1188–1196. URL: https://proceedings.mlr.press/v32/le14.html (дата обращения: 15.10.2023).

7. **Word2Vec: Эмбеддинги слов** // Deep Machine Learning: образовательная платформа. URL: https://deepmachinelearning.ru/docs/Neural-networks/Embeddings/Word2vec (дата обращения: 15.10.2023).