---
title: 'Лекция 26: Трансформеры'
subtitle: Машинное обучение и анализ данных
author: Красников А. С.
date: '2025-04-17'
date-modified: last-modified
institute: МГТУ им. Н.Э. Баумана

format:
  revealjs:
    lang: ru
    scrollable: true
    transition: fade
    width: 1920
    height: 1080
---


## Что такое трансформер?

- Нейросетевая модель для задач `many-to-many` (seq2seq)
- Обрабатывает последовательности:
  - Текст (слова)
  - Видео (кадры)
  - Звук (частоты)
- Примеры задач:
  - Машинный перевод
  - Распознавание речи
  - Прогнозирование временных рядов
  - Генерация музыки

> ChatGPT = Chat Generative Pretrained Transformer


## За пределами many-to-many

- Элементы трансформера (например, self-attention) используются в:
  - Классификации (one-to-one)
  - Генерации изображений (one-to-many)
  - Классификации текста (many-to-one)
- **Визуальный трансформер**: обработка изображений как последовательностей фрагментов.


## Архитектура трансформера
:::: {.columns}
::: {.column width="50%"}
Состоит из:

- **Кодировщик (Encoder)** – обрабатывает входную последовательность
- **Декодировщик (Decoder)** – генерирует выходную последовательность

> Отказ от рекуррентности → параллельная обработка → ускорение обучения.
:::

::: {.column width="50%"}
![Архитектура трансформера](./img/transformer-architecture.png)
:::

::::

## Кодировщик

- Вход: последовательность токенов (слов) с эмбеддингами
- **Позиционное кодирование**: сохранение информации о порядке токенов
- Выход: эмбеддинги, уточнённые с учётом контекста всей последовательности
- Состоит из `N` блоков (например, `N=6`)


## Декодировщик

- Вход: выходная последовательность (например, корректный перевод)
- Режимы:
  - **Обучение**: teacher forcing (используется правильный перевод)
  - **Применение**: авторегрессионная генерация (токены генерируются последовательно)
- Состоит из `N` блоков (например, `N=6`)


## Пример машинного перевода

**Вход (русский):**

1. [Город располагается на берегу реки EOS]
2. [В центре находится парк EOS PAD]
3. [Популярное место EOS PAD PAD PAD]

**Выход (английский):**

1. [BOS The city is located on the river bank EOS]
2. [BOS There is a park in the center EOS PAD]
3. [BOS A popular place EOS PAD PAD PAD PAD PAD]



## Преимущества трансформера

- Параллельная обработка последовательностей
- Эффективное использование механизма внимания
- Быстрое обучение на GPU
- Лучшее качество по сравнению с RNN/LSTM


# Кодировщик трансформера


## Основные компоненты архитектуры
- Принимает последовательность эмбеддингов
- Возвращает уточнённые эмбеддинги с учётом контекста
- Состоит из N последовательных блоков кодировщика (в оригинале N=6)


## Блок кодировщика
:::: {.columns}
::: {.column width="50%"}

- Обрабатывает $D$-мерные эмбеддинги (в оригинале $D=512$)
- Добавляет позиционное кодирование (только для первого блока)
- Последовательно уточняет эмбеддинги
:::

::: {.column width="50%"}
![Схема блока кодировщика](./img/encoder-first-block.png)
:::

::::


## Структура блока кодировщика
:::: {.columns}
::: {.column width="50%"}

1. Блок самовнимания (self-attention)
2. Сумма входного и выходного эмбеддингов (ResNet-like)
3. Послойная нормализация (LayerNorm)
4. Двухслойный персептрон (Feed Forward)
5. Ещё одна сумма с нормализацией
6. Послойная нормализация (LayerNorm).
:::

::: {.column width="50%"}
![Схема блока кодировщика](./img/encoder-first-block.png)
:::

::::


## Feed Forward
Нелинейное преобразование:
$$
\mathbf{y} = \text{ReLU}(\mathbf{x}W_1 + \mathbf{b}_1)W_2 + \mathbf{b}_2
$$

- $\mathbf{x} \in \mathbb{R}^{1 \times D}$ - входной эмбеддинг
- $\mathbf{y} \in \mathbb{R}^{1 \times D}$ - преобразованный эмбеддинг
- $W_1, W_2$ - обучаемые матрицы
- $\mathbf{b}_1, \mathbf{b}_2$ - вектора смещений



## Self-attention


:::: {.columns}
::: {.column width="50%"}
Генерирует для каждого токена:

- Запрос (query) $\mathbf{q} = \mathbf{x}W^Q$
- Ключ (key) $\mathbf{k} = \mathbf{x}W^K$
- Значение (value) $\mathbf{v} = \mathbf{x}W^V$
:::

::: {.column width="50%"}
![Принцип самовнимания](./img/self-attention.png)
:::

::::


## Матричная запись самовнимания

$$
Y = \text{softmax}\left(\frac{1}{\sqrt{d}}QK^T\right)V
$$

- $Q = XW^Q$ - матрица запросов
- $K = XW^K$ - матрица ключей
- $V = XW^V$ - матрица значений

![Матричные вычисления самовнимания](./img/transformer-QKV.png)


## Multi-head self-attention

8 голов внимания с разными весами:
$$
Z = \text{concat}[\text{head}_1, ..., \text{head}_8]W^O
$$
Каждая голова учится фокусироваться на разных аспектах данных

![Многоголовое внимание](./img/transformer-multi-head-output.png)



## Сравнение с RNN

| Характеристика | RNN | Transformer |
|----------------|-----|-------------|
| Сложность | $O(T D^2)$ | $O(T D^2 + T^2 D)$ |
| Путь информации | $O(T)$ шагов | $O(1)$ прямой доступ |
| Память | $D$ измерений | $T \times D$ измерений |



## Итог
- Блок кодировщика уточняет эмбеддинги через:
  - Self-attention (агрегация информации)
  - Feed Forward (нелинейные преобразования)
- Используется 6 последовательных блоков
- Первый блок получает позиционное кодирование


# Позиционное кодирование


## Проблема позиционной информации
- Блоки трансформера работают независимо для каждого токена
- Параллельные вычисления ускоряют работу, но теряют позиционную информацию
- Пример: "mother loves daughter" ≠ "daughter loves mother"


## Решение: позиционное кодирование
- Каждой позиции `pos` сопоставляется D-мерный эмбеддинг
- Эмбеддинги позиций прибавляются к эмбеддингам токенов
- Используется только в первом блоке кодировщика/декодировщика


## Формула позиционного кодирования

Для позиции `p` и размерности `D`:
$$
\begin{aligned}
\mathbf{e}^p_{2i} &= \sin\left(\frac{pos}{10000^{2i/D}}\right) \\
\mathbf{e}^p_{2i+1} &= \cos\left(\frac{pos}{10000^{2i/D}}\right)
\end{aligned}
$$

- Чётные элементы: синусы
- Нечётные элементы: косинусы
- Разные периоды от $2\pi$ до $10000\cdot 2\pi$

---

## Визуализация позиционного кодирования

![Матрица позиционного кодирования](./img/positional-encoding.png)

- Каждый столбец соответствует позиции токена
- Взаимно однозначное соответствие позиция-эмбеддинг


## Аналогия с двоичным кодированием
Позиционное кодирование аналогично двоичному представлению:
$$
0\to(0,0,0,\cdots) \\
1\to(1,0,0,\cdots) \\
2\to(0,1,0,\cdots) \\
3\to(1,1,0,\cdots) \\
\cdots
$$
Но использует непрерывные синусоидальные функции вместо бинарных значений


## Ключевые свойства
1. Уникальность: каждой позиции соответствует уникальный эмбеддинг
2. Линейность: относительные позиции могут быть выведены моделью
3. Обобщение: работает для последовательностей любой длины


# Декодировщик трансформера


## Основная задача
- Принимает эмбеддинги уже сгенерированной последовательности ($M$).
- Выдает распределение вероятностей для следующего элемента.
- Пример: машинный перевод (вход — исходный язык, выход — целевой язык).


## Архитектура декодировщика
- Состоит из $N$ блоков (в оригинале $N=6$).
- Каждый блок уточняет эмбеддинги с учетом контекста.
- После последнего блока:
  - Линейный слой.
  - SoftMax-преобразование.
- Генерация продолжается до токена [EOS].

---

## Блок декодировщика

:::: {.columns}
::: {.column width="50%"}
- Состоит из:
  1. **Маскированного самовнимания** (не позволяет смотреть на будущие токены).
  2. **Кросс-внимания** (связь с выходом кодировщика).
  3. Полносвязной сети.
- Residual connections и LayerNorm после каждого этапа.
:::
::: {.column width="50%"}
![Схема блока](img/2-block-transformer.png)
:::
::::


## Маскированное самовнимание
- Запрещает доступ к будущим позициям ($t+1, t+2, ...$).
- Реализация: добавление $-\infty$ к запрещенным позициям перед SoftMax.
- Пример для позиции $t$:
  Веса внимания: [0.2, 0.5, 0.3, -∞, -∞]


## Кросс-внимание (encoder-decoder)
- **Запросы (Q)** — из выходной последовательности.
- **Ключи (K) и значения (V)** — из входной последовательности.
- Позволяет "смотреть" на эмбеддинги исходного текста.


## Отличия от кодировщика
| **Кодировщик**               | **Декодировщик**               |
|------------------------------|--------------------------------|
| Обрабатывает входной текст   | Генерирует выходной текст      |
| Полное самовнимание          | Маскированное самовнимание     |
| Нет кросс-внимания           | Есть кросс-внимание            |


## Типы внимания в трансформере

![Виды внимания](img/attention-types.png)

1. **Самовнимание в кодировщике**: токены видят все входные токены.
2. **Маскированное внимание**: токены видят только прошлые позиции.
3. **Кросс-внимание**: связь выходных токенов с входными.


# Обучение трансформера


## Контекст внедрения
- **Первое применение**: задача машинного перевода (2017).
- **Преимущества над RNN**:
  - Высокая точность.
  - Полный отказ от рекуррентных вычислений.
  - Легкая **параллелизация** на GPU/TPU.
- Результат: сокращение времени обучения и улучшение качества перевода.


## Параллелизация вычислений
- Все токены обрабатываются **одновременно** (в отличие от RNN).
- Матричные операции вместо последовательных шагов:
  - Self-attention: $Q \cdot K^T$ за один проход.
  - Ускорение обучения в 3-5 раз для больших датасетов.

## Регуляризация: Dropout
Применялся в двух местах:

1. **Остаточные блоки**:
   - Отбрасывание элементов преобразованного эмбеддинга перед суммированием.
   - Вероятность: $p = 0.1$.
2. **Механизмы внимания**:
   - Отбрасывание части эмбеддингов при вычислении весов внимания.


## Label Smoothing
- **Цель**: Улучшение устойчивости модели к переобучению.
- **Способ**:
  - Вместо 100% вероятности для правильного класса:
    $$y_{\text{smoothed}} = (1 - \epsilon) \cdot y_{\text{true}} + \epsilon \cdot \frac{1}{K},$$
    где $\epsilon = 0.1$, $K$ — число классов.
- **Эффект**:
  - Уменьшение уверенности модели в "идеальных" ответах.
  - Улучшение качества для неоднозначных переводов.

---

## Итоги обучения
| **Аспект**         | **Реализация**                          | **Эффект**                          |
|---------------------|-----------------------------------------|--------------------------------------|
| Параллелизация      | Матричные операции                      | Сокращение времени обучения         |
| Регуляризация       | Dropout ($p=0.1$) в двух слоях          | Снижение переобучения               |
| Сглаживание меток   | $\epsilon = 0.1$ для всех классов       | Устойчивость к шуму в данных        |


## Литература

1. Kim Junsu. MetaCodeDeep: Attention & Transformer [Электронный ресурс] / Kim Junsu // Velog. — URL: https://velog.io/@kimjunsu97/MetaCodeDeep-Learning-NLP-Attention-Transformer (дата обращения: 08.04.2025).

2. Позиционное кодирование в трансформерах [Электронный ресурс] // Kikaben.com. — URL: https://kikaben.com/transformers-positional-encoding/ (дата обращения: 08.04.2025).

3. Vaswani A. Attention is all you need // Advances in Neural Information Processing Systems. — 2017. — URL: https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf (дата обращения: 08.04.2025).

4. Demystifying the Transformer Revolution in NLP [Электронный ресурс] // Medium. — URL: https://medium.com/analytics-vidhya/attention-is-all-you-need-demystifying-the-transformer-revolution-in-nlp-68a2a5fbd95b (дата обращения: 08.04.2025).
