---
title: 'Лекция 27: Обработка графов'
subtitle: Машинное обучение и анализ данных
author: Красников А. С.
date: '2025-04-17'
date-modified: last-modified
institute: МГТУ им. Н.Э. Баумана

format:
  revealjs:
    lang: ru
    scrollable: true
    transition: fade
    width: 1920
    height: 1080
---



# Типы графов

## Ненаправленный граф

:::: {.columns}
::: {.column width="50%"}
Ребра не имеют направления

- **Примеры**:
  - Молекулы (атомы и химические связи)
  - Социальные сети (взаимная дружба)
  - Электрические схемы
  - Туристические карты
:::
::: {.column width="50%"}
![Неориентированный граф](./img/01-graph.png)
:::
::::


## Направленный граф
:::: {.columns}
::: {.column width="50%"}
 Ребра имеют четкое направление (от источника к цели)

- **Примеры**:
  - Подписки в соцсетях
  - Логистические сети
  - Потоки данных
:::
::: {.column width="50%"}
  ![Ориентированный граф](./img/02-directed-graph.png)
:::
::::



## Смешанный граф
- **Особенность**: Содержит как направленные, так и ненаправленные ребра
- **Применение**:
  - Транспортные сети (односторонние/двусторонние дороги)
  - Энергосистемы (линии передачи)


## Ассоциированные данные

:::: {.columns}
::: {.column width="50%"}
- **Вершины**: D-мерные векторы признаков
  $$
  v_i → \mathbf{h}_i \in \mathbb{R}^D
  $$
- **Ребра**: Векторы данных
  $$
  c_j → \mathbf{e}_j \in \mathbb{R}^{D_e}
  $$
- **Граф целиком**: Глобальный вектор признаков
  $$
  \mathbf{g} \in \mathbb{R}^{D_g}
  $$
:::
::: {.column width="50%"}
![](./img/03-graph-with-data.png)
:::
::::




## Граф ребер
:::: {.columns}
::: {.column width="50%"}

  1. Ребра исходного графа → вершины нового
  2. Соединения → общие узлы исходного графа
:::
::: {.column width="50%"}
![Граф ребер](./img/04-edge-graph.png)
:::
::::



## Мультиграфы

:::: {.columns}
::: {.column width="50%"}
Множественные ребра разных типов между узлами

- Социальные взаимодействия (лайки, комментарии)
- Логистика (разные виды транспорта)
:::
::: {.column width="50%"}
![Мультиграф](./img/05-multi-graph.png)
:::
::::


## Гетерогенные графы

:::: {.columns}
::: {.column width="50%"}
Узлы и ребра разных типов

- Торговые системы (пользователи-товары-продавцы)
- Графы знаний (персоны-события-компании)

:::
::: {.column width="50%"}
![Гетерогенный граф](./img/06-heterogeneous-multi-graph.png)
:::
::::


# Матричное представление

## Матрица смежности

:::: {.columns}
::: {.column width="40%"}
**Ненаправленный граф**:
  $$
  a_{ij} =
  \begin{cases}
  1, & \text{если} v_i \text{и} v_j \text{соединены} \\
  0, & \text{иначе}
  \end{cases}
  $$
:::
::: {.column width="30%"}
![Ненаправленный граф](./img/01-graph.png)
:::
::: {.column width="30%"}
$$A = \begin{bmatrix}
0 & 1 & 1 & 0 & 0 \\
1 & 0 & 0 & 1 & 0 \\
1 & 0 & 0 & 1 & 0 \\
0 & 1 & 1 & 0 & 1 \\
0 & 0 & 0 & 1 & 0
\end{bmatrix}
$$
:::
::::

:::: {.columns}
::: {.column width="40%"}
**Направленный граф**:
  $$
  a_{ij} =
  \begin{cases}
  1, & \text{ребро из } v_j \text{ в } v_i \\
  0, & \text{иначе}
  \end{cases}
  $$
:::
::: {.column width="30%"}
![Направленный граф](./img/02-directed-graph.png)
:::
::: {.column width="30%"}
$$A =
\begin{bmatrix}
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0
\end{bmatrix}
$$
:::
::::



## Матрица степеней


:::: {.columns}
::: {.column width="40%"}
Диагональная матрица со степенями вершин
:::
::: {.column width="30%"}
![Направленный граф](./img/01-graph.png)
:::
::: {.column width="30%"}
$$D =
\begin{bmatrix}
2 & 0 & 0 & 0 & 0 \\
0 & 2 & 0 & 0 & 0 \\
0 & 0 & 2 & 0 & 0 \\
0 & 0 & 0 & 3 & 0 \\
0 & 0 & 0 & 0 & 1
\end{bmatrix}
$$
:::
::::


# Изоморфизм графов

## Основные понятия
:::: {.columns}
::: {.column width="50%"}
- **Изоморфные графы**: Графы, идентичные с точностью до перенумерации вершин
- **Ключевое свойство**: Результаты обработки графа не должны зависеть от нумерации вершин
:::
::: {.column width="50%"}
![Изоморфные графы](./img/07-equivalent-graph.png)
:::
::::


## Матрица перестановок
:::: {.columns}
::: {.column width="50%"}
Матрица $P \in \mathbb{R}^{N \times N}$ задающая перенумерацию вершин:

$$
p_{ij} =
\begin{cases}
1, & \text{если } i\text{-й узел перенумеруется } j\text{-м узлом} \\
0, & \text{иначе}
\end{cases}
$$

При этом: $P \cdot P^T = P^T \cdot P = I$
:::
::: {.column width="50%"}
Для перенумерации $1 \to 2', 2 \to 3', 3 \to 1'$:

$$
P = \begin{pmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{pmatrix}
$$
:::
::::

![Изоморфные графы](./img/07-equivalent-graph.png)


## Преобразование матрицы данных при изоморфизме

Правило преобразования:
$$ H' = H \cdot P $$

**Пример**:
Исходные данные:
$$
v_1 = \begin{pmatrix}
1 \\
2 \\
3 \\
4
\end{pmatrix},
v_2 = \begin{pmatrix}
10 \\
20 \\
30 \\
40
\end{pmatrix},
v_3 = \begin{pmatrix}
100 \\
200 \\
300 \\
400
\end{pmatrix}
\rightarrow
H = \begin{pmatrix}
1 & 10 & 100 \\
2 & 20 & 200 \\
3 & 30 & 300 \\
4 & 40 & 400
\end{pmatrix}, \quad
P = \begin{pmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{pmatrix}
$$

После преобразования:
$$
H' = H \cdot P =
\begin{pmatrix}
1 & 10 & 100 \\
2 & 20 & 200 \\
3 & 30 & 300 \\
4 & 40 & 400
\end{pmatrix}
\cdot
\begin{pmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{pmatrix}
=
\begin{pmatrix}
100 & 1 & 10 \\
200 & 2 & 20 \\
300 & 3 & 30 \\
400 & 4 & 40
\end{pmatrix}
$$

## Преобразование матрицы смежности при изоморфизме

:::: {.columns}
::: {.column width="50%"}
Прямое преобразование:
$$ A' = P^T \cdot A \cdot P $$

:::
::: {.column width="50%"}
Обратное преобразование:
$$ A = P \cdot A' \cdot P^T $$

:::
::::

![Изоморфные графы](./img/07-equivalent-graph.png)

:::: {.columns}
::: {.column width="30%"}
Исходная матрица:
$$
A = \begin{pmatrix}
0 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 0
\end{pmatrix}
$$
:::
::: {.column width="30%"}
$$
P = \begin{pmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{pmatrix}
$$
:::

::: {.column width="30%"}
Преобразованная матрица
$$
A' = \begin{pmatrix}
0 & 0 & 1 \\
0 & 0 & 1 \\
1 & 1 & 0
\end{pmatrix}
$$

:::
::::



## Преобразование матрицы степеней при изоморфизме
Правило преобразования:
$$ D' = P^T \cdot D \cdot P $$

**Пример**:
Исходная матрица:
$$
D = \begin{pmatrix}
1 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 1
\end{pmatrix}
$$

После преобразования:
$$
D' = \begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 2
\end{pmatrix}
$$

## Ключевые выводы
1. Все матричные представления графа преобразуются согласованно при изоморфизме
2. Основные формулы преобразований:
   - Данные: $H' = H \cdot P$
   - Смежность: $A' = P^T \cdot A \cdot P$
   - Степени: $D' = P^T \cdot D \cdot P$
3. Обратные преобразования выполняются с помощью $P^T$



# Задачи на графах

## Типы задач на графах

1. **Предсказание свойств узлов**
   - Известны свойства части узлов → восстановить для остальных.
2. **Предсказание свойств рёбер**
   - Сводится к задаче для узлов через преобразование в граф рёбер.
3. **Восстановление отсутствующих рёбер**
   - Предсказание связей в неполном графе.
4. **Классификация графов**
   - Множество небольших графов → предсказание свойств для новых.


## Примеры прикладных задач

- **Социальные сети**:
  - Узлы: пользователи; рёбра: взаимодействия.
  - Задачи: предсказание интересов, обнаружение ботов, рекомендации друзей.
- **Граф цитирований**:
  - Узлы: научные работы; рёбра: ссылки.
  - Задачи: оценка тематики, рекомендации релевантных статей.

## Примеры прикладных задач

- **Химические соединения**:
  - Узлы: атомы; рёбра: связи.
  - Задачи: предсказание свойств веществ (токсичность, растворимость).
- **Интернет как граф**:
  - Узлы: веб-страницы; рёбра: гиперссылки.
  - Задачи: оценка важности страниц, SEO-оптимизация.

## Примеры прикладных задач
- **Компьютерные сети**:
  - Оптимизация маршрутов, обнаружение атак.
- **Городское планирование**:
  - Прогнозирование пробок, оптимизация транспортных потоков.
- **Логистика**:
  - Управление цепочками поставок, прогнозирование задержек.


## Примеры прикладных задач

- **Граф знаний**: Музеи и картины (из Wikidata).
  - Узлы: объекты культуры; рёбра: связи.

    ![Граф знаний](./img/08-knowledge-graph.jpg)
- **Задачи**:
  - Поиск связей между событиями, группировка по темам.

---

## Примеры прикладных задач

### Программный код как граф

- **Типы графов**:
  - Call graph (вызовы функций), Data flow graph (зависимости переменных).
- **Задачи**:
  - Поиск уязвимостей, автоматическое комментирование, генерация тестов.


# Решение задач на графах

## Построение эмбеддингов вершин
- **Начальные эмбеддинги**:
  $$ v_i \to \mathbf{h}^0_i \in \mathbb{R}^{D}, \quad i=1,2,...N $$
  - Примеры данных: пол, возраст, one-hot кодирование, геометрические характеристики.
- **Контекстное уточнение**:
  - Использование графовых свёрточных сетей (GCN).
  - Итеративное обновление:
    $$ \{\mathbf{h}_i^1\}_i \to \{\mathbf{h}_i^2\}_i \to ... \to \{\mathbf{h}_i\}_i $$

---

## Задачи на графе целиком
- **Инвариантность к перенумерации вершин**:
  $$ \hat{y}(H) \equiv \hat{y}(H \cdot P) $$
- **Регрессия**:
  $$ \hat{y}(G) = w_0 + \mathbf{w}^T \cdot H \cdot \mathbf{i}/N $$
- **Бинарная классификация**:
  $$ \hat{y}(G) = \sigma(w_0 + \mathbf{w}^T \cdot H \cdot \mathbf{i}/N) $$
- **Многоклассовая классификация**:
  $$ \mathbf{p}(y|G) = \text{SoftMax}(\mathbf{w}_0 + W \cdot H \cdot \mathbf{i}/N) $$

## Альтернативные подходы
- **Суммирование эмбеддингов**:
  $$ \hat{y}(G) = \mathbf{w}^T \cdot H \cdot \mathbf{i} $$
- **Максимизирующий пулинг**:
  $$ \hat{y}(G) = \max(\mathbf{h}_1, \mathbf{h}_2, ..., \mathbf{h}_N) $$


## Обработка отдельных вершин
- **Эквивариантность**:
  $$ \hat{y}(H \cdot P) = \hat{y}(H) \cdot P $$
- **Регрессия для вершины**:
  $$ \hat{y}(\mathbf{h}_i) = w_0 + \mathbf{w}^T \mathbf{h}_i $$
- **Классификация**:
  $$ p(y=1|\mathbf{h}_i) = \sigma(\mathbf{w}^T \mathbf{h}_i) $$
- **Многоклассовый SoftMax**:
  $$ \mathbf{p}(y|\mathbf{h}_i) = \text{SoftMax}(\mathbf{w}_0 + W \cdot \mathbf{h}_i) $$

## Восстановление рёбер
- **Вероятность связи**:
  $$ p((v_i, v_j) \text{ - связаны}) = \sigma(\mathbf{h}_i^T \mathbf{h}_j) $$
- **Пороговое условие**:
  $$ p > t \implies \text{соединить } v_i, v_j $$
- **Расширенная модель**:
  $$ p = \sigma(w_0 + \mathbf{h}_i^T W \mathbf{h}_j) $$
  - Симметричная матрица весов: $W = W^T$.



#  Свёрточные графовые сети (Graph Convolutional Networks)


## Введение в графовые сети
**Цель**:
Обновление признаков вершин с учётом структуры графа.

**Основные этапы**:

- Итеративное обновление эмбеддингов вершин.
- Учёт информации от соседних узлов.
- Аналогия со свёрточными слоями в CNN.

**Формула базового обновления**:
$$
h_i^{k+1} = f\left(\sum_{j \in \mathcal{N}(i)} w_j h_j^k + b\right)
$$


## Алгоритм передачи сообщений

**Шаги**:

1. **Агрегация**: Сбор данных от соседей.
2. **Обновление**: Комбинирование собранных данных с текущим состоянием узла.

**Матричная форма**:
$$
H^{k+1} = F(H^k, A, W^k)
$$
где $A$ — матрица смежности, $W^k$ — веса на слое $k$.

---

## Функции агрегации

**Варианты агрегации**:

- **Сумма**:
  $$
  \text{Aggregate}(\cdot) = \sum_{m \in \mathcal{N}(n)} h_m^k
  $$
- **Усреднение**:
  $$
  \text{Aggregate}(\cdot) = \frac{1}{|\mathcal{N}(n)|} \sum_{m \in \mathcal{N}(n)} h_m^k
  $$
- **Максимум**:
  $$
  \text{Aggregate}(\cdot) = \max_{m \in \mathcal{N}(n)} h_m^k
  $$



##  Примеры и визуализации
**Иллюстрация передачи сообщений**:

![**Итерация 0**: Исходные признаки.](./img/08-message-passing-0.png)

---

![**Итерация 1**: Учёт соседей 1-го порядка.](./img/08-message-passing-1.png)

---

![**Итерация 2**: Учёт соседей 2-го порядка.](./img/08-message-passing-2.png)


---

![**Итерация 3**: Учёт соседей 3-го порядка.](./img/08-message-passing-3.png)


# Обучение графовых нейросетей


## Генерация минибатчей
**Цель**: Ускорение обучения через параллелизацию.

**Подходы**:

- **Для графов целиком**:
  Объединение нескольких графов в *мета-граф* (несвязанные компоненты).
  - После обработки мета-графа результаты разделяются для классификации/регрессии.

- **Для узлов**:
  Выбор случайных вершин и их подграфов (все узлы на расстоянии $\leqslant K$).
  - Проблема: высокосвязные графы → подграфы слишком большие.


## Классификация/регрессия узлов
:::: {.columns}
::: {.column width="50%"}
**Проблема**:
При $K$ итерациях подграфы могут охватывать весь исходный граф.

**Решение**:

1. **Разбиение графа**:
   - Использование алгоритмов разделения графа на несвязанные подграфы.
   - Пример:
2. **Аналог дропаута**:
   - Использование случайных $S$ соседей на каждом шаге.
   - При обучении: $S$ ограничено.
   - При инференсе: учитываются все соседи с перевзвешиванием.
:::
::: {.column width="50%"}
![Разбиение графа](./img/10-graph-partitioning.png)
:::
::::


## Регуляризация

- **Стандартные методы**:
  - $L_1$- и $L_2$-регуляризация весов.
  - Сокращение числа слоёв/нейронов.
- **Специфичные для графов**:
  - **Weight sharing**:
    Параметры слоёв (итераций передачи сообщений) делаются одинаковыми.
  - **DropOut для рёбер**:
    Случайное удаление рёбер во время обучения.

**Пример регуляризации**:
$$
\text{Loss} = \text{Основная функция} + \lambda \cdot (\|W\|_1 + \|W\|_2)
$$


# Альтернативные эмбеддинги вершин в графах

## Эмбеддинги на основе случайных блужданий

- **Основная идея:**
  - Генерация путей обхода графа (случайные блуждания).
  - Сближение эмбеддингов вершин, находящихся рядом.
- **Преимущества:**
  - Экономия памяти по сравнению с message passing в GCN.


## DeepWalk

**Алгоритм:**

1. Из каждой вершины запускается \( K \times M \) случайных блужданий.
2. Используется **SkipGram** (Word2Vec):
   - Окно шириной \( 2W + 1 \).
   - Предсказание соседних вершин по центральной.

**Пример:**
- Путь: $v_1 \to v_2 \to v_3 \to v_4$ → окна: $[v_1, v_2, v_3]$, $[v_2, v_3, v_4]$.


## Node2Vec

- **Улучшение DeepWalk:**
  - **Управляемые блуждания** с параметрами \( p \) и \( q \):
    - \( p \): Вероятность возврата к предыдущей вершине.
    - \( q \): Вероятность перехода к дальним вершинам.
- **Стратегии обхода:**
  - **BFS** (ширина): Изучение локальной структуры (хабы, связующие узлы).
  - **DFS** (глубина): Выделение сообществ.
- **Формула перехода:**
$$
p(v \to x | v, t) = \begin{cases}
\gamma \cdot \frac{1}{p}, & \text{если } x = t, \\
\gamma \cdot 1, & \text{если } d(t, x) = 1, \\
\gamma \cdot \frac{1}{q}, & \text{если } d(t, x) = 2.
\end{cases}
$$



## Node2Vec

- **Обход в глубину:** Кластеризация по сообществам.
- **Обход в ширину:** Кластеризация по структурным ролям.

![Кластеризация](./img/13-node-clustering-examples.jpg)


## Автокодировщики

- **Идея:**
  - Строка матрицы смежности → вход автокодировщика.
  - Скрытый слой → компактный эмбеддинг.

- **Особенности:**
  - Требуется регуляризация для борьбы с переобучением.
  - Подход из статьи **Structural Deep Network Embedding**.


## Совмещение подходов

- **Варианты интеграции с GCN:**
  1. **Инициализация эмбеддингов:** Геометрические эмбеддинги как начальные признаки.
  2. **Дополнение признаков:** Комбинация исходных признаков и геометрических эмбеддингов.
- **Преимущества:**
  - Улучшение качества предсказаний за счет структурной информации.


## Итоги

- DeepWalk и Node2Vec эффективны для работы с крупными графами.
- Автокодировщики дают гибкость, но требуют аккуратной настройки.
- Совмещение методов повышает точность моделей.


## Литература


1. **Граф (математика)** [Электронный ресурс] // Википедия. – URL: https://ru.wikipedia.org/wiki/Граф_(математика) (дата обращения: 01.10.2023).
2. Prince S.J.D. *Understanding Deep Learning*. – 2023.
3. **Графы знаний на Wikidata** [Электронный ресурс] // Wikidata. – URL: https://www.wikidata.org/wiki/User:Fuzheado/queries (дата обращения: 01.10.2023).
4. Shin D., Kim I. *Deep image understanding using multilayered contexts*.
5. **Исходный материал** [Электронный ресурс] // deepmachinelearning.ru. – URL: https://deepmachinelearning.ru (дата обращения: 01.10.2023).
6. Gilmer J. et al. *Neural Message Passing for Quantum Chemistry* // ICML. – 2017.
7. Kipf T.N., Welling M. *Semi-Supervised Classification with GCNs* [Электронный ресурс] // arXiv. – 2016. – URL: https://arxiv.org/abs/1609.02907 (дата обращения: 01.10.2023).
8. Veličković P. et al. *Graph Attention Networks* [Электронный ресурс] // arXiv. – 2017. – URL: https://arxiv.org/abs/1710.10903 (дата обращения: 01.10.2023).
9. Perozzi B. et al. *DeepWalk: Online Learning of Social Representations* [Электронный ресурс] // arXiv. – 2014. – URL: https://arxiv.org/pdf/1403.6652 (дата обращения: 01.10.2023).
10. Grover A., Leskovec J. *node2vec: Scalable Feature Learning for Networks* [Электронный ресурс] // arXiv. – 2016. – URL: https://arxiv.org/pdf/1607.00653 (дата обращения: 01.10.2023).
11. Wang D. et al. *Structural Deep Network Embedding* // KDD. – 2016. – URL: https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf (дата обращения: 01.10.2023).
