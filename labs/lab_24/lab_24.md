---
jupyter:
  jupytext:
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.16.7
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Текстовые эмбеддинги и их применение

Продолжительность работы: - 4 часа.

Мягкий дедлайн (10 баллов): 10.04.2025

Жесткий дедлайн (5 баллов): 24.04.2025


**Цель работы**: Изучить основные методы представления текстовой информации в машинном обучении.


Для всех заданий используйте датасет:

```python
text = ["кошка любит молоко",
        "собака охраняет дом",
        "ребенок играет в мяч",
        "программист пишет код",
        "студент сдает лабораторную",
]
```

# Задание 1: One-Hot кодирование
Реализуйте функцию one_hot_encode, которая преобразует список слов в one-hot векторы.
Пример: словарь ['кот', 'собака', 'бежит'] → вектор для 'собака' → [0, 1, 0]

```python
def one_hot_encode(text, vocabulary):
    # Здесь должен быть ваш код
    pass
```

```python
# Пример решения:
def one_hot_encode(word, vocabulary):
    vector = [0] * len(vocabulary)
    index = vocabulary.index(word)
    vector[index] = 1
    return vector

sample_text = ["кот", "собака", "бежит"]
sample_vocab = list(set(sample_text))

print("Пример one-hot кодирования:", one_hot_encode("собака", sample_vocab))
```

# Задание 2: Матрица совстречаемости

Создайте матрицу совстречаемости слов для текста с окном context_size=2.
Используйте нормализацию логарифмированием.

```python
import numpy as np

def create_cooccurrence_matrix(corpus, vocab, context_size=2):
    # Здесь должен быть ваш код
    return matrix
```

# Задание 3: Снижение размерности с помощью SVD

Примените Truncated SVD к матрице совстречаемости и визуализируйте 
первые 2 компоненты для 10 слов.

```python
from sklearn.decomposition import TruncatedSVD
import matplotlib.pyplot as plt

def visualize_svd(matrix, words):
    # Здесь должен быть ваш код
    pass
```

# Задание 4: Реализация Skip-Gram модели

Постройте модель Skip-Gram с использованием Keras.
Используйте слой Embedding и оптимизатор Adam.

```python
from keras.models import Sequential
from keras.layers import Embedding, Dense

def build_skip_gram(vocab_size, embedding_dim):
    # Здесь должен быть ваш код
    return model
```

# Задание 5: Работа с предобученными эмбеддингами

Загрузите предобученные векторы GloVe и найдите топ-5 ближайших 
слов к заданному слову.

```python
from gensim.models import KeyedVectors

def find_similar_words(word, model_path, topn=5):
    # Здесь должен быть ваш код
    pass
```

## Контрольные вопросы:
1. В чем основные недостатки one-hot кодирования для представления текста?
2. Как выбор размера окна влияет на свойства матрицы совстречаемости?
3. Чем отличается подход LSA от word2vec в построении эмбеддингов?
4. Объясните разницу между архитектурами Skip-Gram и CBOW
5. Какие метрики используются для оценки качества векторных представлений?
6. Как методы вроде PPMI помогают улучшить матрицу совстречаемости?
7. Почему при визуализации SVD мы используем только первые 2 компоненты?
8. Какие преимущества дают предобученные эмбеддинги (GloVe, FastText)?
9. Как обрабатываются OOV (out-of-vocabulary) слова в разных подходах?
10. Объясните на примере, как можно использовать эмбеддинги для классификации текстов


## Дополнительные практические вопросы:

1. Почему в задании 3 для 10 слов визуализация может показать кластеры?
2. Как бы вы модифицировали Skip-Gram модель для работы с русским языком?
3. Какие проблемы могут возникнуть при применении этих методов к коротким текстам?
4. Как выбрать оптимальную размерность для эмбеддингов на практике?
5. Объясните, какие слова будут ближе в пространстве эмбеддингов для вашего датасета.
